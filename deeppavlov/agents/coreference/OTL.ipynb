{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "from os.path import join, isdir, basename\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "  return [item for sublist in l for item in sublist]\n",
    "\n",
    "class DocumentState(object):\n",
    "    def __init__(self):\n",
    "        self.doc_key = None\n",
    "        self.text = []\n",
    "        self.text_speakers = []\n",
    "        self.speakers = []\n",
    "        self.sentences = []\n",
    "        self.clusters = collections.defaultdict(list)\n",
    "        self.stacks = collections.defaultdict(list)\n",
    "\n",
    "    def assert_empty(self):\n",
    "        assert self.doc_key is None\n",
    "        assert len(self.text) == 0\n",
    "        assert len(self.text_speakers) == 0\n",
    "        assert len(self.sentences) == 0\n",
    "        assert len(self.speakers) == 0\n",
    "        assert len(self.clusters) == 0\n",
    "        assert len(self.stacks) == 0\n",
    "\n",
    "    def assert_finalizable(self):\n",
    "        assert self.doc_key is not None\n",
    "        assert len(self.text) == 0\n",
    "        assert len(self.text_speakers) == 0\n",
    "        assert len(self.sentences) > 0\n",
    "        assert len(self.speakers) > 0\n",
    "        assert all(len(s) == 0 for s in self.stacks.values())\n",
    "\n",
    "    def finalize(self):\n",
    "        merged_clusters = []\n",
    "        for c1 in self.clusters.values():\n",
    "            existing = None\n",
    "            for m in c1:\n",
    "                for c2 in merged_clusters:\n",
    "                    if m in c2:\n",
    "                        existing = c2\n",
    "                        break\n",
    "                if existing is not None:\n",
    "                    break\n",
    "            if existing is not None:\n",
    "                print(\"Merging clusters (shouldn't happen very often.)\")\n",
    "                existing.update(c1)\n",
    "            else:\n",
    "                merged_clusters.append(set(c1))\n",
    "        merged_clusters = [list(c) for c in merged_clusters]\n",
    "        all_mentions = flatten(merged_clusters)\n",
    "        # print len(all_mentions), len(set(all_mentions))\n",
    "\n",
    "        if len(all_mentions) != len(set(all_mentions)):\n",
    "            c = Counter(all_mentions)\n",
    "            for x in c:\n",
    "                if c[x] > 1:\n",
    "                    z = x\n",
    "                    break\n",
    "            for i in range(len(all_mentions)):\n",
    "                if all_mentions[i] == z:\n",
    "                    all_mentions.remove(all_mentions[i])\n",
    "                    break\n",
    "        assert len(all_mentions) == len(set(all_mentions))\n",
    "\n",
    "        return {\n",
    "            \"doc_key\": self.doc_key,\n",
    "            \"sentences\": self.sentences,\n",
    "            \"speakers\": self.speakers,\n",
    "            \"clusters\": merged_clusters\n",
    "        }\n",
    "\n",
    "def normalize_word(word):\n",
    "    if word == \"/.\" or word == \"/?\":\n",
    "        return word[1:]\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def conll2modeldata(data):\n",
    "    document_state = DocumentState()\n",
    "    document_state.assert_empty()\n",
    "    document_state.doc_key = \"{}_{}\".format(data['doc_id'][0], data['part_id'][0])\n",
    "    for i in range(len(data['doc_id'])):\n",
    "        word = normalize_word(data['word'][i])\n",
    "        coref = data['coreference'][i]\n",
    "        speaker = data['speaker'][i]\n",
    "        word_index = i + 1\n",
    "        document_state.text.append(word)\n",
    "        document_state.text_speakers.append(speaker)\n",
    "\n",
    "        if coref != \"-\":\n",
    "            for segment in coref.split(\"|\"):\n",
    "                if segment[0] == \"(\":\n",
    "                    if segment[-1] == \")\":\n",
    "                        cluster_id = int(segment[1:-1]) # Need Int\n",
    "                        document_state.clusters[cluster_id].append((word_index, word_index))\n",
    "                    else:\n",
    "                        cluster_id = int(segment[1:])\n",
    "                        document_state.stacks[cluster_id].append(word_index)\n",
    "                else:\n",
    "                    cluster_id = int(segment[:-1])\n",
    "                    start = document_state.stacks[cluster_id].pop()\n",
    "                    document_state.clusters[cluster_id].append((start, word_index))\n",
    "        else:                 \n",
    "            if (data['part_of_speech'][i] == 'End_of_sentence'):\n",
    "                document_state.sentences.append(tuple(document_state.text))\n",
    "                del document_state.text[:]\n",
    "                document_state.speakers.append(tuple(document_state.text_speakers))\n",
    "                del document_state.text_speakers[:]\n",
    "                continue\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    document_state.assert_finalizable()\n",
    "    return document_state.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conll2dict(iter_id, conll, agent, mode, epoch_done=False):\n",
    "    data = {'doc_id': [],\n",
    "            'part_id': [],\n",
    "            'word_number': [],\n",
    "            'word': [],\n",
    "            'part_of_speech': [],\n",
    "            'parse_bit': [],\n",
    "            'lemma': [],\n",
    "            'sense': [],\n",
    "            'speaker': [],\n",
    "            'entiti': [],\n",
    "            'predict': [],\n",
    "            'coreference': [],\n",
    "            'iter_id': iter_id,\n",
    "            'id': agent,\n",
    "            'epoch_done': epoch_done,\n",
    "            'mode': mode}\n",
    "\n",
    "    with open(conll, 'r') as f:\n",
    "        for line in f:\n",
    "            row = line.split('\\t')\n",
    "            if row[0].startswith('#'):\n",
    "                continue\n",
    "            elif row[0] == '\\n':\n",
    "                data['doc_id'].append('bc')\n",
    "                data['part_id'].append('0')\n",
    "                data['word_number'].append('0')\n",
    "                data['word'].append('SeNt')\n",
    "                data['part_of_speech'].append('End_of_sentence')\n",
    "                data['parse_bit'].append('-')\n",
    "                data['lemma'].append('-')\n",
    "                data['sense'].append('-')\n",
    "                data['speaker'].append('-')\n",
    "                data['entiti'].append('-')\n",
    "                data['predict'].append('-')\n",
    "                data['coreference'].append('-')\n",
    "            else:\n",
    "                assert len(row) >= 12\n",
    "                data['doc_id'].append(row[0])\n",
    "                data['part_id'].append(row[1])\n",
    "                data['word_number'].append(row[2])\n",
    "                data['word'].append(row[3])\n",
    "                data['part_of_speech'].append(row[4])\n",
    "                data['parse_bit'].append(row[5])\n",
    "                data['lemma'].append(row[6])\n",
    "                data['sense'].append(row[7])\n",
    "                data['speaker'].append(row[8])\n",
    "                data['entiti'].append(row[9])\n",
    "                data['predict'].append(row[10])\n",
    "                data['coreference'].append(row[11][0:-1])\n",
    "        f.close()\n",
    "    return data\n",
    "\n",
    "def dict2conll(data, predict):\n",
    "    #\n",
    "    with open(predict, 'w') as CoNLL:\n",
    "        for i in range(len(data['doc_id'])):\n",
    "            if i == 0:\n",
    "                CoNLL.write('#begin document ({}); part {}\\n'.format(data['doc_id'][i], data[\"part_id\"][i]))\n",
    "                CoNLL.write(u'{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(data['doc_id'][i],\n",
    "                                                    data[\"part_id\"][i],\n",
    "                                                    data[\"word_number\"][i],\n",
    "                                                    data[\"word\"][i],\n",
    "                                                    data[\"part_of_speech\"][i],\n",
    "                                                    data[\"parse_bit\"][i],\n",
    "                                                    data[\"lemma\"][i],\n",
    "                                                    data[\"sense\"][i],\n",
    "                                                    data[\"speaker\"][i],\n",
    "                                                    data[\"entiti\"][i],\n",
    "                                                    data[\"predict\"][i],\n",
    "                                                    data[\"coreference\"][i]))\n",
    "            elif i == len(data['doc_id'])-1 and data['part_of_speech'][i] == 'End_of_sentence':\n",
    "                CoNLL.write('#end document\\n')\n",
    "            elif data['part_of_speech'][i] == 'End_of_sentence':\n",
    "                continue\n",
    "            else:\n",
    "                if data['doc_id'][i] == data['doc_id'][i+1]:\n",
    "                    CoNLL.write(u'{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(data['doc_id'][i],\n",
    "                                                        data[\"part_id\"][i],\n",
    "                                                        data[\"word_number\"][i],\n",
    "                                                        data[\"word\"][i],\n",
    "                                                        data[\"part_of_speech\"][i],\n",
    "                                                        data[\"parse_bit\"][i],\n",
    "                                                        data[\"lemma\"][i],\n",
    "                                                        data[\"sense\"][i],\n",
    "                                                        data[\"speaker\"][i],\n",
    "                                                        data[\"entiti\"][i],\n",
    "                                                        data[\"predict\"][i],\n",
    "                                                        data[\"coreference\"][i]))\n",
    "                elif data['part_of_speech'][i] == 'End_of_sentence':\n",
    "                    continue\n",
    "                else:\n",
    "                    CoNLL.write(u'{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(data['doc_id'][i],\n",
    "                                                    data[\"part_id\"][i],\n",
    "                                                    data[\"word_number\"][i],\n",
    "                                                    data[\"word\"][i],\n",
    "                                                    data[\"part_of_speech\"][i],\n",
    "                                                    data[\"parse_bit\"][i],\n",
    "                                                    data[\"lemma\"][i],\n",
    "                                                    data[\"sense\"][i],\n",
    "                                                    data[\"speaker\"][i],\n",
    "                                                    data[\"entiti\"][i],\n",
    "                                                    data[\"predict\"][i],\n",
    "                                                    data[\"coreference\"][i]))\n",
    "                    CoNLL.write('\\n')\n",
    "        CoNLL.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conll = '/home/petrov/coreference_kpi/coreference/src/parlai/data/coreference/russian/train/0.russian.v4_conll'\n",
    "data = conll2dict(0, conll, 'agent', 'test', epoch_done=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(data['coreference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = './test.conll'\n",
    "dict2conll(data, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = conll2modeldata(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_key': 'bc1_0', 'sentences': [('Во', 'время', 'своих', 'прогулок', 'в', 'окрестностях', 'Симеиза', 'я', 'обратил', 'внимание', 'на', 'одинокую', 'дачу', ',', 'стоявшую', 'на', 'крутом', 'склоне', 'горы', '.', 'SeNt'), ('К', 'этой', 'даче', 'не', 'было', 'проведено', 'даже', 'дороги', '.', 'SeNt'), ('Кругом', 'она', 'была', 'обнесена', 'высоким', 'забором', ',', 'с', 'единственной', 'низкой', 'калиткой', ',', 'которая', 'всегда', 'была', 'плотно', 'прикрыта', '.', 'SeNt'), ('И', 'ни', 'куста', 'зелени', ',', 'ни', 'дерева', 'не', 'виднелось', 'над', 'забором', '.', 'SeNt'), ('Кругом', 'дачи', '-', 'голые', 'уступы', 'желтоватых', 'скал', ';', 'меж', 'ними', 'кое', '-', 'где', 'росли', 'чахлые', 'можжевельники', 'и', 'низкорослые', ',', 'кривые', 'горные', 'сосны', '.', 'SeNt'), ('\"', 'Что', 'за', 'фантазия', 'пришла', 'кому', '-', 'то', 'в', 'голову', 'поселиться', 'на', 'этом', 'диком', ',', 'голом', 'утесе', '?', 'Да', 'и', 'живет', 'ли', 'там', 'кто', '-', 'нибудь', '?', '\"', '-', 'думал', 'я', ',', 'бродя', 'вокруг', 'дачи', '.', 'SeNt'), ('Я', 'еще', 'никогда', 'не', 'видел', ',', 'чтобы', 'кто', '-', 'нибудь', 'выходил', 'оттуда', '.', 'SeNt'), ('Любопытство', 'мое', 'было', 'так', 'велико', ',', 'что', 'я', ',', 'признаюсь', ',', 'пытался', 'заглянуть', 'на', 'двор', 'таинственного', 'жилища', ',', 'взобравшись', 'на', 'вышележащие', 'скалы', '.', 'SeNt'), ('Но', 'дача', 'была', 'так', 'расположена', ',', 'что', ',', 'откуда', 'бы', 'я', 'ни', 'заходил', ',', 'я', 'мог', 'видеть', 'только', 'небольшой', 'угол', 'двора', '.', 'SeNt'), ('Он', 'был', 'так', 'же', 'пуст', 'и', 'невозделан', ',', 'как', 'и', 'окружающая', 'местность', '.', 'SeNt'), ('Однако', 'после', 'нескольких', 'дней', 'наблюдений', 'мне', 'удалось', 'заметить', ',', 'что', 'по', 'двору', 'прошла', 'какая', '-', 'то', 'пожилая', 'женщина', 'в', 'черном', '.', 'SeNt'), ('Это', 'еще', 'больше', 'заинтересовало', 'меня', '.', 'SeNt'), ('Если', 'там', 'живут', 'люди', ',', 'то', 'должны', 'же', 'они', 'поддерживать', 'хоть', 'какую', '-', 'нибудь', 'связь', 'с', 'внешним', 'миром', ',', 'ну', ',', 'хотя', 'бы', 'ходить', 'на', 'базар', 'за', 'продуктами', '!', 'Я', 'стал', 'наводить', 'справки', 'среди', 'своих', 'знакомых', ',', 'и', ',', 'наконец', ',', 'мне', 'удалось', 'удовлетворить', 'мое', 'любопытство', '.', 'SeNt'), ('Правда', ',', 'никто', 'не', 'знал', 'достоверно', 'об', 'обитателях', 'дачи', ',', 'но', 'один', 'знакомый', 'сообщил', 'мне', ',', 'что', ',', 'по', 'слухам', ',', 'там', 'живет', 'профессор', 'Вагнер', '.', 'SeNt'), ('Профессор', 'Вагнер', '!', 'Этого', 'было', 'достаточно', ',', 'чтобы', 'совершенно', 'приковать', 'мое', 'внимание', 'к', 'даче', '.', 'SeNt'), ('Мне', 'во', 'что', 'бы', 'то', 'ни', 'стало', 'захотелось', 'увидеть', 'необычайного', 'человека', ',', 'наделавшего', 'столько', 'шума', 'своими', 'изобретениями', '.', 'SeNt'), ('Но', 'как', '?', '.', 'SeNt'), ('.', 'SeNt'), ('Я', 'буквально', 'стал', 'шпионить', 'за', 'дачей', '.', 'SeNt'), ('Я', 'чувствовал', ',', 'что', 'это', 'было', 'нехорошо', ',', 'и', 'все', '-', 'таки', 'продолжал', 'свои', 'наблюдения', ',', 'целыми', 'часами', 'в', 'разное', 'время', 'дня', 'и', 'даже', 'ночи', 'просиживая', 'за', 'можжевеловым', 'кустом', ',', 'недалеко', 'от', 'дачи', '.', 'SeNt'), ('Говорят', ',', 'если', 'человек', 'неотступно', 'преследует', 'одну', 'цель', ',', 'то', 'рано', 'или', 'поздно', 'он', 'достигнет', 'ее', '.', 'SeNt'), ('Как', '-', 'то', 'рано', 'утром', ',', 'когда', 'только', 'что', 'рассвело', ',', 'я', 'вдруг', 'услышал', ',', 'что', 'заветная', 'дверь', 'в', 'высоком', 'заборе', 'скрипнула', '.', 'SeNt'), ('Я', 'весь', 'насторожился', ',', 'сжался', 'и', 'затаив', 'дыхание', 'стал', 'следить', ',', 'что', 'будет', 'дальше', '.', 'SeNt'), ('Дверь', 'открылась', '.', 'SeNt'), ('Высокий', 'человек', ',', 'с', 'румяным', 'лицом', ',', 'русой', 'бородой', 'и', 'нависшими', 'усами', 'вышел', 'и', 'внимательно', 'осмотрелся', 'вокруг', '.', 'SeNt'), ('Конечно', ',', 'это', 'он', ',', 'профессор', 'Вагнер', '!', 'Убедившись', ',', 'что', 'вокруг', 'никого', 'нет', ',', 'он', 'стал', 'медленно', 'подниматься', 'вверх', ',', 'дошел', 'до', 'небольшой', 'горной', 'площадки', 'и', 'начал', 'заниматься', 'там', 'какими', '-', 'то', 'совершенно', 'непонятными', 'для', 'меня', 'упражнениями', '.', 'SeNt'), ('На', 'площадке', 'были', 'разбросаны', 'камни', 'различной', 'величины', '.', 'SeNt'), ('Вагнер', 'подходил', 'к', 'ним', 'и', 'делал', 'попытки', 'поднять', 'их', ',', 'затем', ',', 'осторожно', 'ступая', ',', 'переходил', 'на', 'новое', 'место', 'и', 'опять', 'брался', 'за', 'камни', '.', 'SeNt'), ('Но', 'все', 'они', 'были', 'так', 'велики', 'и', 'тяжелы', ',', 'что', 'даже', 'профессиональный', 'атлет', 'едва', 'ли', 'смог', 'бы', 'сдвинуть', 'их', 'с', 'места', '.', 'SeNt'), ('\"', 'Что', 'за', 'странная', 'забава!\"', '-', 'подумал', 'я', '.', 'SeNt'), ('И', 'вдруг', 'я', 'был', 'так', 'поражен', ',', 'что', 'не', 'мог', 'сдержать', 'невольное', 'восклицание', '.', 'SeNt'), ('Произошло', 'что', '-', 'то', 'невероятное', ':', 'профессор', 'Вагнер', 'подошел', 'к', 'огромному', 'обломку', 'скалы', ',', 'величиною', 'более', 'человеческого', 'роста', ',', 'взял', 'за', 'выступавший', 'острый', 'край', 'и', 'поднял', 'обломок', 'с', 'такой', 'легкостью', ',', 'как', 'если', 'бы', 'это', 'был', 'кусок', 'картона', '.', 'SeNt'), ('Вытянув', 'руку', ',', 'он', 'начал', 'описывать', 'дуги', 'этим', 'обломком', 'скалы', '.', 'SeNt'), ('Я', 'не', 'знал', ',', 'что', 'подумать', '.', 'SeNt'), ('Или', 'Вагнер', 'обладал', 'сверхъестественной', 'силой', '...', 'но', 'тогда', 'почему', 'он', 'не', 'мог', 'поднять', 'небольшие', 'сравнительно', 'камни', ',', 'или', '...', 'Я', 'не', 'успел', 'додумать', 'свою', 'мысль', ',', 'как', 'новый', 'фокус', 'Вагнера', 'еще', 'больше', 'поразил', 'меня', '.', 'SeNt'), ('Вагнер', 'бросил', 'глыбу', 'вверх', ',', 'как', 'маленький', 'камешек', ',', 'и', 'она', 'полетела', ',', 'поднявшись', 'на', 'высоту', 'двух', 'десятков', 'метров', '.', 'SeNt'), ('С', 'волнением', 'ожидал', 'я', ',', 'как', 'грохнет', 'эта', 'глыба', 'на', 'землю', '.', 'SeNt'), ('Но', 'обломок', 'падал', 'обратно', 'довольно', 'медленно', '.', 'SeNt'), ('Я', 'насчитал', 'десять', 'секунд', ',', 'прежде', 'чем', 'глыба', 'опустилась', 'вниз', '.', 'SeNt'), ('И', ',', 'когда', 'она', 'была', 'над', 'землей', 'на', 'высоте', 'человеческого', 'роста', ',', 'Вагнер', 'подставил', 'руку', ',', 'поймал', 'и', 'удержал', 'глыбу', ',', 'причем', 'рука', 'его', 'даже', 'не', 'дрогнула', '.', 'SeNt'), ('Хо-хо-хо', '!', '-', 'весело', 'баском', 'рассмеялся', 'Вагнер', 'и', 'далеко', 'отшвырнул', 'от', 'себя', 'глыбу', '.', 'SeNt'), ('Она', ',', 'пролетев', 'некоторое', 'время', 'параллельно', 'земле', ',', 'вдруг', 'круто', 'изменила', 'линию', 'полета', 'на', 'отвесную', ',', 'быстро', 'упала', 'и', 'со', 'страшным', 'грохотом', 'разлетелась', 'в', 'куски', '.', 'SeNt'), ('Хо-хо-хо', '!', '-', 'опять', 'рассмеялся', 'Вагнер', 'и', 'сделал', 'необычайный', 'прыжок', '.', 'SeNt'), ('Поднявшись', 'метра', 'на', 'четыре', ',', 'он', 'пролетел', 'вдоль', 'площадки', 'в', 'мою', 'сторону', '.', 'SeNt'), ('Он', ',', 'очевидно', ',', 'не', 'рассчитал', 'прыжка', ',', 'так', 'как', 'с', 'ним', 'случилась', 'такая', 'же', 'история', ',', 'что', 'и', 'с', 'глыбой', ':', 'неожиданно', 'он', 'стал', 'быстро', 'падать', '.', 'SeNt'), ('И', 'если', 'бы', 'не', 'откос', ',', 'куда', 'он', 'упал', ',', 'Вагнер', ',', 'вероятно', ',', 'расшибся', 'бы', 'насмерть', '.', 'SeNt'), ('Он', 'упал', 'неподалеку', 'от', 'меня', ',', 'по', 'другую', 'сторону', 'можжевелового', 'куста', ',', 'застонал', 'и', 'выбранился', ',', 'ухватившись', 'за', 'колено', '.', 'SeNt'), ('Погладив', 'ушибленное', 'место', ',', 'он', 'сделал', 'попытку', 'встать', 'и', 'вновь', 'застонал', '.', 'SeNt'), ('После', 'некоторого', 'колебания', 'я', 'решил', 'обнаружить', 'свое', 'присутствие', 'и', 'подать', 'ему', 'помощь', '.', 'SeNt'), ('Вы', 'очень', 'расшиблись', '?', 'Не', 'помочь', 'ли', 'вам', '?', '-', 'спросил', 'я', ',', 'выходя', 'из', 'куста', '.', 'SeNt'), ('По', '-', 'видимому', ',', 'мое', 'появление', 'не', 'удивило', 'профессора', '.', 'SeNt'), ('По', 'крайней', 'мере', 'он', 'ничем', 'не', 'проявил', 'его', '.', 'SeNt'), ('Нет', ',', 'благодарю', 'вас,', '-', 'спокойно', 'ответил', 'он,', '-', 'я', 'сам', 'пойду', '.', 'SeNt'), ('-', 'И', 'он', 'сделал', 'новую', 'попытку', 'встать', '.', 'SeNt'), ('Лицо', 'его', 'исказилось', 'от', 'боли', '.', 'SeNt'), ('Он', 'даже', 'откинулся', 'назад', '.', 'SeNt'), ('Нога', 'в', 'колене', 'быстро', 'пухла', '.', 'SeNt'), ('Было', 'очевидно', ',', 'что', 'без', 'посторонней', 'помощи', 'ему', 'не', 'обойтись', '.', 'SeNt'), ('И', 'я', 'стал', 'действовать', 'решительно', '.', 'SeNt')], 'speakers': [('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', '-')], 'clusters': [[(672, 672), (928, 928), (173, 173), (205, 205), (314, 314), (506, 506), (947, 947), (140, 140), (263, 263), (1031, 1031), (3, 3), (125, 125), (431, 431), (645, 645), (580, 580), (836, 836), (892, 892), (258, 258), (354, 354), (270, 270), (958, 958), (177, 177), (575, 575), (146, 146), (418, 418), (320, 320), (346, 346), (686, 686), (273, 273), (291, 291), (731, 731), (118, 118), (925, 925), (978, 978), (676, 676), (713, 713), (8, 8), (226, 226), (367, 367)], [(351, 351), (154, 155), (285, 285), (317, 317), (122, 122), (12, 13), (164, 164), (386, 386), (33, 33), (23, 24), (65, 65)], [(36, 37), (426, 427), (61, 61)], [(423, 427), (44, 44), (40, 42), (447, 447)], [(73, 73), (67, 70)], [(211, 211), (153, 155), (183, 183), (186, 186)], [(232, 232), (237, 237)], [(819, 819), (755, 755), (851, 851), (943, 943), (1005, 1005), (876, 876), (519, 519), (888, 888), (991, 991), (451, 462), (879, 879), (300, 301), (636, 636), (766, 766), (654, 654), (962, 962), (475, 476), (689, 689), (831, 831), (335, 335), (783, 783), (1025, 1025), (473, 473), (999, 999), (329, 330), (913, 913), (984, 984), (599, 600), (304, 305), (485, 485), (662, 662), (682, 682), (778, 778), (982, 982), (932, 932), (840, 840), (936, 936), (968, 968), (863, 863)], [(381, 382), (951, 951), (897, 898)], [(392, 392), (402, 402)], [(404, 404), (395, 396)], [(493, 495), (511, 511), (834, 834)], [(527, 527), (514, 516), (522, 522)], [(547, 547), (563, 563), (542, 542)], [(784, 784), (699, 699), (619, 619), (691, 691), (717, 718), (746, 746), (762, 762), (724, 724), (787, 787), (860, 860), (738, 738), (640, 642), (603, 605)], [(757, 757), (765, 765)], [(910, 911), (1013, 1013), (906, 906)]]}\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [00:00<00:00, 292.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 315.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n",
      "Merging clusters (shouldn't happen very often.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "from os import listdir\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = '/home/petrov/coreference_kpi/coreference/src/parlai/data/coreference/russian/train/'\n",
    "conll_list = listdir(path)\n",
    "for x in tqdm(conll_list):\n",
    "    data = conll2dict(0, join(path,x), 'agent', 'test', epoch_done=False)\n",
    "    a = conll2modeldata(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path1 = '../../../src/parlai/data/coreference/russian/vocab/char_vocab.russian.txt'\n",
    "path2 = '/home/petrov/coreference_kpi/code/e2e-coref/vocab/char_vocab.russian.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 197\n"
     ]
    }
   ],
   "source": [
    "with open(path1,'r') as new:\n",
    "    x = new.readlines()\n",
    "with open(path2,'r') as old:\n",
    "    y = old.readlines()\n",
    "\n",
    "print(len(sorted(set(x))), len(sorted(set(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n"
     ]
    }
   ],
   "source": [
    "z = list(set(x)&set(y))\n",
    "print(len(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if ' \\n' in set(x):\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n', ' \\n'}\n"
     ]
    }
   ],
   "source": [
    "print(set(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = [' \\n','\\n']\n",
    "with open(path2,'r+') as file:\n",
    "    for c in z:\n",
    "        file.write(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
