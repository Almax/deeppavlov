{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "from os.path import join, isdir, basename\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "  return [item for sublist in l for item in sublist]\n",
    "\n",
    "class DocumentState(object):\n",
    "    def __init__(self):\n",
    "        self.doc_key = None\n",
    "        self.text = []\n",
    "        self.text_speakers = []\n",
    "        self.speakers = []\n",
    "        self.sentences = []\n",
    "        self.clusters = collections.defaultdict(list)\n",
    "        self.stacks = collections.defaultdict(list)\n",
    "\n",
    "    def assert_empty(self):\n",
    "        assert self.doc_key is None\n",
    "        assert len(self.text) == 0\n",
    "        assert len(self.text_speakers) == 0\n",
    "        assert len(self.sentences) == 0\n",
    "        assert len(self.speakers) == 0\n",
    "        assert len(self.clusters) == 0\n",
    "        assert len(self.stacks) == 0\n",
    "\n",
    "    def assert_finalizable(self):\n",
    "        assert self.doc_key is not None\n",
    "        assert len(self.text) == 0\n",
    "        assert len(self.text_speakers) == 0\n",
    "        assert len(self.sentences) > 0\n",
    "        assert len(self.speakers) > 0\n",
    "        assert all(len(s) == 0 for s in self.stacks.values())\n",
    "\n",
    "    def finalize(self):\n",
    "        merged_clusters = []\n",
    "        for c1 in self.clusters.values():\n",
    "            existing = None\n",
    "            for m in c1:\n",
    "                for c2 in merged_clusters:\n",
    "                    if m in c2:\n",
    "                        existing = c2\n",
    "                        break\n",
    "                if existing is not None:\n",
    "                    break\n",
    "            if existing is not None:\n",
    "                print(\"Merging clusters (shouldn't happen very often.)\")\n",
    "                existing.update(c1)\n",
    "            else:\n",
    "                merged_clusters.append(set(c1))\n",
    "        merged_clusters = [list(c) for c in merged_clusters]\n",
    "        all_mentions = flatten(merged_clusters)\n",
    "        # print len(all_mentions), len(set(all_mentions))\n",
    "\n",
    "        if len(all_mentions) != len(set(all_mentions)):\n",
    "            c = Counter(all_mentions)\n",
    "            for x in c:\n",
    "                if c[x] > 1:\n",
    "                    z = x\n",
    "                    break\n",
    "            for i in range(len(all_mentions)):\n",
    "                if all_mentions[i] == z:\n",
    "                    all_mentions.remove(all_mentions[i])\n",
    "                    break\n",
    "        assert len(all_mentions) == len(set(all_mentions))\n",
    "\n",
    "        return {\n",
    "            \"doc_key\": self.doc_key,\n",
    "            \"sentences\": self.sentences,\n",
    "            \"speakers\": self.speakers,\n",
    "            \"clusters\": merged_clusters\n",
    "        }\n",
    "\n",
    "def normalize_word(word):\n",
    "    if word == \"/.\" or word == \"/?\":\n",
    "        return word[1:]\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def conll2modeldata(data):\n",
    "    document_state = DocumentState()\n",
    "    document_state.assert_empty()\n",
    "    document_state.doc_key = \"{}_{}\".format(data['doc_id'][0], data['part_id'][0])\n",
    "    for i in range(len(data['doc_id'])):\n",
    "        word = normalize_word(data['word'][i])\n",
    "        coref = data['coreference'][i]\n",
    "        speaker = data['speaker'][i]\n",
    "        word_index = i + 1\n",
    "        document_state.text.append(word)\n",
    "        document_state.text_speakers.append(speaker)\n",
    "\n",
    "        if coref != \"-\":\n",
    "            for segment in coref.split(\"|\"):\n",
    "                if segment[0] == \"(\":\n",
    "                    if segment[-1] == \")\":\n",
    "                        cluster_id = int(segment[1:-1])\n",
    "                        document_state.clusters[cluster_id].append((word_index, word_index))\n",
    "                    else:\n",
    "                        cluster_id = int(segment[1:])\n",
    "                        document_state.stacks[cluster_id].append(word_index)\n",
    "                else:\n",
    "                    cluster_id = int(segment[:-1])\n",
    "                    start = document_state.stacks[cluster_id].pop()\n",
    "                    document_state.clusters[cluster_id].append((start, word_index))\n",
    "        else:                 \n",
    "            if (data['part_of_speech'][i] == 'SEN'):\n",
    "                document_state.sentences.append(tuple(document_state.text))\n",
    "                del document_state.text[:]\n",
    "                document_state.speakers.append(tuple(document_state.text_speakers))\n",
    "                del document_state.text_speakers[:]\n",
    "                continue\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    document_state.assert_finalizable()\n",
    "    return document_state.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conll2dict(iter_id, conll, agent, mode, epoch_done=False):\n",
    "    data = {'doc_id': [],\n",
    "            'part_id': [],\n",
    "            'word_number': [],\n",
    "            'word': [],\n",
    "            'part_of_speech': [],\n",
    "            'parse_bit': [],\n",
    "            'lemma': [],\n",
    "            'sense': [],\n",
    "            'speaker': [],\n",
    "            'entiti': [],\n",
    "            'predict': [],\n",
    "            'coreference': [],\n",
    "            'iter_id': iter_id,\n",
    "            'id': agent,\n",
    "            'epoch_done': epoch_done,\n",
    "            'mode': mode}\n",
    "\n",
    "    with open(conll, 'r') as f:\n",
    "        for line in f:\n",
    "            row = line.split('\\t')\n",
    "            if row[0].startswith('#') or row[0] == '\\n':\n",
    "                pass\n",
    "            else:\n",
    "                assert len(row) >= 12\n",
    "                data['doc_id'].append(row[0])\n",
    "                data['part_id'].append(row[1])\n",
    "                data['word_number'].append(row[2])\n",
    "                data['word'].append(row[3])\n",
    "                data['part_of_speech'].append(row[4])\n",
    "                data['parse_bit'].append(row[5])\n",
    "                data['lemma'].append(row[6])\n",
    "                data['sense'].append(row[7])\n",
    "                data['speaker'].append(row[8])\n",
    "                data['entiti'].append(row[9])\n",
    "                data['predict'].append(row[10])\n",
    "                data['coreference'].append(row[11][0:-1])\n",
    "        f.close()\n",
    "    return data\n",
    "\n",
    "def dict2conll(data, predict):\n",
    "    #\n",
    "    with open(predict, 'w') as CoNLL:\n",
    "        for i in range(len(data['doc_id'])):\n",
    "            if i == 0:\n",
    "                CoNLL.write('#begin document ({}); part {}\\n'.format(data['doc_id'][i], data[\"part_id\"][i]))\n",
    "                CoNLL.write(u'{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(data['doc_id'][i],\n",
    "                                                    data[\"part_id\"][i],\n",
    "                                                    data[\"word_number\"][i],\n",
    "                                                    data[\"word\"][i],\n",
    "                                                    data[\"part_of_speech\"][i],\n",
    "                                                    data[\"parse_bit\"][i],\n",
    "                                                    data[\"lemma\"][i],\n",
    "                                                    data[\"sense\"][i],\n",
    "                                                    data[\"speaker\"][i],\n",
    "                                                    data[\"entiti\"][i],\n",
    "                                                    data[\"predict\"][i],\n",
    "                                                    data[\"coreference\"][i]))\n",
    "            elif i == len(data['doc_id'])-1:\n",
    "                CoNLL.write(u'{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(data['doc_id'][i],\n",
    "                                                    data[\"part_id\"][i],\n",
    "                                                    data[\"word_number\"][i],\n",
    "                                                    data[\"word\"][i],\n",
    "                                                    data[\"part_of_speech\"][i],\n",
    "                                                    data[\"parse_bit\"][i],\n",
    "                                                    data[\"lemma\"][i],\n",
    "                                                    data[\"sense\"][i],\n",
    "                                                    data[\"speaker\"][i],\n",
    "                                                    data[\"entiti\"][i],\n",
    "                                                    data[\"predict\"][i],\n",
    "                                                    data[\"coreference\"][i]))\n",
    "                CoNLL.write('\\n')\n",
    "                CoNLL.write('#end document\\n')\n",
    "            elif data['part_of_speech'][i] == 'SEN':\n",
    "                CoNLL.write(u'{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(data['doc_id'][i],\n",
    "                                                    data[\"part_id\"][i],\n",
    "                                                    data[\"word_number\"][i],\n",
    "                                                    data[\"word\"][i],\n",
    "                                                    data[\"part_of_speech\"][i],\n",
    "                                                    data[\"parse_bit\"][i],\n",
    "                                                    data[\"lemma\"][i],\n",
    "                                                    data[\"sense\"][i],\n",
    "                                                    data[\"speaker\"][i],\n",
    "                                                    data[\"entiti\"][i],\n",
    "                                                    data[\"predict\"][i],\n",
    "                                                    data[\"coreference\"][i]))\n",
    "                CoNLL.write('\\n')\n",
    "            else:\n",
    "                if data['doc_id'][i] == data['doc_id'][i+1]:\n",
    "                    CoNLL.write(u'{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(data['doc_id'][i],\n",
    "                                                        data[\"part_id\"][i],\n",
    "                                                        data[\"word_number\"][i],\n",
    "                                                        data[\"word\"][i],\n",
    "                                                        data[\"part_of_speech\"][i],\n",
    "                                                        data[\"parse_bit\"][i],\n",
    "                                                        data[\"lemma\"][i],\n",
    "                                                        data[\"sense\"][i],\n",
    "                                                        data[\"speaker\"][i],\n",
    "                                                        data[\"entiti\"][i],\n",
    "                                                        data[\"predict\"][i],\n",
    "                                                        data[\"coreference\"][i]))\n",
    "                    if data[\"word_number\"][i+1] == 0:\n",
    "                        CoNLL.write('\\n')\n",
    "                else:\n",
    "                    CoNLL.write(u'{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(data['doc_id'][i],\n",
    "                                                        data[\"part_id\"][i],\n",
    "                                                        data[\"word_number\"][i],\n",
    "                                                        data[\"word\"][i],\n",
    "                                                        data[\"part_of_speech\"][i],\n",
    "                                                        data[\"parse_bit\"][i],\n",
    "                                                        data[\"lemma\"][i],\n",
    "                                                        data[\"sense\"][i],\n",
    "                                                        data[\"speaker\"][i],\n",
    "                                                        data[\"entiti\"][i],\n",
    "                                                        data[\"predict\"][i],\n",
    "                                                        data[\"coreference\"][i]))\n",
    "                    CoNLL.write('\\n')\n",
    "                    CoNLL.write('#end document\\n')\n",
    "                    CoNLL.write('#begin document ({}); part {}\\n'.format(data['doc_id'][i+1], data[\"part_id\"][i+1]))\n",
    "        CoNLL.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conll = '/home/petrov/coreference_kpi/coreference/src/parlai/data/coreference/russian/train/173.russian.v4_conll'\n",
    "data = conll2dict(0, conll, 'agent', 'test', epoch_done=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = './test.conll'\n",
    "dict2conll(data, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-201e744c3b45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconll2modeldata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-c670f792f36b>\u001b[0m in \u001b[0;36mconll2modeldata\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mdocument_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_finalizable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdocument_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-c670f792f36b>\u001b[0m in \u001b[0;36massert_finalizable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massert_finalizable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_speakers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a = conll2modeldata(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_key': 'bc739_0', 'sentences': [('И', 'все', 'же', 'этот', 'день', 'был', 'днем', 'везения', '.'), ('Охотник', 'остановился', 'у', 'огромного', 'дуба', ',', 'чтобы', 'еще', 'раз', 'свериться', 'с', 'направлением', 'по', 'солнцу', '.'), ('Тело', 'убитого', 'и', 'выпотрошенного', 'олененка', 'оказалось', 'тяжелой', 'ношей', ',', 'это', 'заметно', 'влияло', 'на', 'скорость', 'передвижения', ':', 'уже', 'час', 'назад', 'он', 'должен', 'был', 'выйти', 'к', 'дороге', ',', 'где', 'его', 'ждал', 'темно', '-', 'зеленый', 'УАЗик', '.'), ('И', 'еще', 'это', 'болото', 'с', 'гигантами', '-', 'дубами', ',', 'которое', 'совсем', 'сбило', 'его', 'с', 'толку', ':', 'казалось', ',', 'он', 'шел', 'назад', 'тем', 'же', 'путем', ',', 'но', 'заблудился', '!'), ('Странное', 'болото', 'появилось', 'неизвестно', 'откуда', '.'), ('Близился', 'вечер', '.'), ('День', 'медленно', 'догорал', ',', 'цепляясь', 'солнцем', 'за', 'кроны', 'деревьев', '.'), ('Он', 'шел', ',', 'и', 'был', 'твердо', 'уверен', ',', 'что', 'еще', 'немного', '-', 'и', 'он', 'выберется', '.'), ('Он', 'не', 'паниковал', ',', 'старый', ',', 'опытный', 'охотник', '.'), ('Пока', 'по', 'пояс', 'не', 'провалился', 'в', 'болотную', 'жижу', '.'), ('Он', 'закричал', '.'), ('Услышав', 'крик', ',', 'старик', ',', 'дремлющий', 'перед', 'тлеющим', 'костерком', ',', 'вздрогнул', '.'), ('Он', 'на', 'секунду', 'замер', ',', 'вслушиваясь', 'в', 'вечерний', 'лес', ',', 'потом', 'схватил', 'здоровенную', 'палку', 'и', 'быстро', 'зашагал', 'по', 'направлению', 'к', 'трясине', '.'), ('Старик', 'появился', 'перед', 'ним', 'неожиданно', '.'), ('Охотник', ',', 'погрузившийся', 'уже', 'по', 'грудь', ',', 'даже', 'не', 'заметил', 'его', 'приближения', '.'), ('-',), ('Ну', ',', 'старик', ',', 'а', 'ты', 'меня', 'напугал', '.'), ('Так', 'тихо', 'подкрался', '.'), ('Нагни', '-', 'ка', 'мне', 'вон', 'ту', 'березку', ',', 'видишь', ',', 'увяз', 'я', 'малость', '.'), ('Старик', 'не', 'шелохнулся', '.'), ('Он', 'увидел', 'изуродованное', 'тело', 'маленького', 'зверя', ',', 'кровавым', 'куском', 'валявшееся', 'позади', 'охотника', '.'), ('-',), ('Эй', ',', 'папаша', ',', 'ты', 'что', ',', 'заснул', '?'), ('Я', 'к', 'тебе', 'обращаюсь', '.'), ('Нагни', 'эту', 'чертову', 'березку', ',', 'чтобы', 'я', 'смог', 'выбраться', '!'), ('Старик', 'не', 'обратил', 'на', 'это', 'никакого', 'внимания', '.'), ('Он', 'посмотрел', 'на', 'охотника', 'и', 'отрицательно', 'покачал', 'головой', '.'), ('-',), ('Да', 'помоги', 'же', 'мне', ',', 'старый', 'говн', '...', '-', 'дальнейшее', 'потонуло', 'в', 'неразборчивом', 'бульканье', ',', 'грязная', 'вонючая', 'жижа', 'плеснулась', 'ему', 'в', 'рот', '.'), ('Трясина', 'уже', 'почти', 'затянула', 'его', 'к', 'себе', '.'), ('Он', 'крепче', 'схватил', 'руками', 'карабин', ',', 'погружавшийся', 'вместе', 'с', 'ним', '.'), ('И', 'тогда', 'старик', 'спросил', 'его', ':', '-'), ('Ты', 'веришь', 'в', 'сказки', '?'), ('-',), ('Какие', ',', 'в', 'жопу', ',', 'сказки', '?!'), ('Да', 'я', 'сдохну', 'через', 'минуту', '!!!'), ('Кидай', 'скорее', 'палку', ',', 'Ну', ',', 'же', '!'), ('-',), ('А', 'зря', 'ты', 'не', 'веришь', 'в', 'сказки', '.'), ('А', 'ведь', 'я', '-', 'леший', '.'), ('Я', 'хозяин', 'этого', 'леса', ',', 'который', 'ты', 'осквернил', 'своим', 'бессмысленным', 'и', 'жестоким', 'убийством', '.'), ('-',), ('Да', 'ты', 'что', ',', 'старый', 'козел', ',', 'спятил', '?!'), ('-',), ('Ты', 'совершил', 'зло', '.'), ('И', 'ты', 'умрешь', '.'), ('-',), ('Ну', ',', 'получай', ',', 'сука', '!'), ('Охотник', 'с', 'трудом', 'вырвал', ',', 'увязший', 'в', 'трясине', ',', 'карабин', 'быстро', 'снял', 'его', 'с', 'предохранителя', 'и', ',', 'почти', 'не', 'целясь', ',', 'выстрелил', 'в', 'старика', '.'), ('Пуля', 'прошила', 'лешего', 'навылет', '.'), ('Старик', 'даже', 'не', 'вздрогнул', ':', 'призрак', ',', 'сказочное', 'существо', ',', 'неуязвимое', 'для', 'всесильных', 'пуль', '.'), ('Очертания', 'старика', 'стали', 'размываться', '.'), ('Он', 'уходил', '.'), ('Охотник', 'задергался', 'пытаясь', 'еще', 'раз', 'самостоятельно', 'дотянутся', 'до', 'спасительного', 'дерева', '.'), ('Этим', 'он', 'только', 'ускорил', 'свое', 'погружение', '.'), ('Последнее', ',', 'что', 'он', 'увидел', ',', 'потрясло', 'его', 'и', 'на', 'мгновение', 'заставило', 'забыть', 'о', 'своей', 'гибели', ':', 'молодой', 'олененок', 'бежал', 'прочь', ',', 'покидая', 'болото', 'и', 'смерть', '.'), ('И', 'все', 'же', 'этот', 'день', 'был', 'днем', 'везения', '.')], 'speakers': [('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1',), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1',), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1',), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1',), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1',), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1',), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1',), ('spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1'), ('spk1',), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1'), ('spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1', 'spk1')], 'clusters': [[(400, 400), (480, 480), (77, 77), (203, 203), (262, 262), (122, 122), (213, 213), (277, 277), (44, 44), (52, 52), (71, 71), (140, 140), (180, 180), (468, 468), (500, 500), (251, 251), (315, 315), (381, 381), (324, 324), (493, 493), (383, 383), (363, 363), (289, 291), (415, 415), (303, 303), (333, 333), (221, 221), (347, 347), (10, 10), (106, 106), (330, 330), (483, 483), (119, 119), (183, 183), (311, 311), (405, 405), (489, 489), (239, 239), (287, 287)], [(68, 68), (61, 66), (87, 88)], [(224, 224), (394, 395), (371, 371), (461, 461), (442, 442), (244, 244), (199, 199), (391, 391), (253, 253), (445, 445), (193, 193), (177, 177), (146, 146), (376, 378), (155, 155), (413, 413), (202, 202), (246, 246), (266, 266), (438, 438), (228, 228), (375, 375), (328, 328), (274, 274), (465, 465)], [(307, 307), (313, 313)], [(377, 378), (380, 380)], [(424, 424), (427, 427)]]}\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
