{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from parlai.core.agents import create_agent\n",
    "from parlai.core.params import ParlaiParser\n",
    "from parlai.core.utils import Timer\n",
    "from parlai.core.worlds import create_task\n",
    "from utils import build_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, isdir\n",
    "import os\n",
    "from deeppavlov.agents.coreference.agents import CoreferenceAgent\n",
    "from deeppavlov.agents.coreference.utils import conll2modeldata\n",
    "from deeppavlov.tasks.coreference.utils import score, dict2conll, conll2dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_eval(agent, opt, datatype, max_exs=-1, write_log=False, valid_world=None):\n",
    "    \"\"\"Eval on validation/test data.\n",
    "    - Agent is the agent to use for the evaluation.\n",
    "    - opt is the options that specific the task, eval_task, etc\n",
    "    - datatype is the datatype to use, such as \"valid\" or \"test\"\n",
    "    - write_log specifies to write metrics to file if the model_file is set\n",
    "    - max_exs limits the number of examples if max_exs > 0\n",
    "    - valid_world can be an existing world which will be reset instead of reinitialized\n",
    "    \"\"\"\n",
    "    print('[ running eval: ' + datatype + ' ]')\n",
    "    opt['datatype'] = datatype\n",
    "    if opt.get('evaltask'):\n",
    "\n",
    "        opt['task'] = opt['evaltask']\n",
    "\n",
    "    if valid_world is None:\n",
    "        valid_world = create_task(opt, agent)\n",
    "    else:\n",
    "        valid_world.reset()\n",
    "    cnt = 0\n",
    "    for _ in valid_world:\n",
    "        valid_world.parley()\n",
    "        if cnt == 0 and opt['display_examples']:\n",
    "            print(valid_world.display() + '\\n~~')\n",
    "            print(valid_world.report())\n",
    "        cnt += opt['batchsize']\n",
    "        if valid_world.epoch_done() or (max_exs > 0 and cnt > max_exs):\n",
    "            # note this max_exs is approximate--some batches won't always be\n",
    "            # full depending on the structure of the data\n",
    "            break\n",
    "    valid_report = valid_world.report()\n",
    "\n",
    "    metrics = datatype + ':' + str(valid_report)\n",
    "    print(metrics)\n",
    "    if write_log and opt['model_file']:\n",
    "        # Write out metrics\n",
    "        f = open(opt['model_file'] + '.' + datatype, 'w')\n",
    "        f.write(metrics + '\\n')\n",
    "        f.close()\n",
    "\n",
    "    return valid_report, valid_world\n",
    "\n",
    "def train_model(opt):\n",
    "    # Possibly build a dictionary (not all models do this).\n",
    "    if opt['dict_build_first'] and 'dict_file' in opt:\n",
    "        if opt['dict_file'] is None and opt.get('pretrained_model'):\n",
    "            opt['dict_file'] = opt['pretrained_model'] + '.dict'\n",
    "        if opt['dict_file'] is None and opt.get('model_file'):\n",
    "            opt['dict_file'] = opt['model_file'] + '.dict'\n",
    "        print(\"[ building dictionary first... ]\")\n",
    "        build_dict.build_dict(opt)\n",
    "    # Create model and assign it to the specified task\n",
    "    agent = create_agent(opt)\n",
    "    if opt['datatype'].split(':')[0] == 'train':\n",
    "        world = create_task(opt, agent)\n",
    "\n",
    "        train_time = Timer()\n",
    "        validate_time = Timer()\n",
    "        log_time = Timer()\n",
    "        print('[ training... ]')\n",
    "        parleys = 0\n",
    "        total_exs = 0\n",
    "        max_exs = opt['num_epochs'] * len(world)\n",
    "        epochs_done = 0\n",
    "        max_parleys = math.ceil(max_exs / opt['batchsize'])\n",
    "        best_metric_name = opt['chosen_metric']\n",
    "        best_metric = 0\n",
    "        impatience = 0\n",
    "        lr_drop_impatience = 0\n",
    "        saved = False\n",
    "        valid_world = None\n",
    "        try:\n",
    "            while True:\n",
    "                world.parley()\n",
    "                parleys += 1\n",
    "                new_epoch = world.epoch_done()\n",
    "                if new_epoch:\n",
    "                    world.reset()\n",
    "                    epochs_done += 1\n",
    "\n",
    "                if opt['num_epochs'] > 0 and parleys >= max_parleys:\n",
    "                    print('[ num_epochs completed: {} ]'.format(opt['num_epochs']))\n",
    "                    break\n",
    "                if 0 < opt['max_train_time'] < train_time.time():\n",
    "                    print('[ max_train_time elapsed: {} ]'.format(train_time.time()))\n",
    "                    break\n",
    "                if (0 < opt['log_every_n_secs'] < log_time.time()) or \\\n",
    "                        (opt['log_every_n_epochs'] > 0 and new_epoch and\n",
    "                                 (epochs_done % opt['log_every_n_epochs']) == 0):\n",
    "                    if opt['display_examples']:\n",
    "                        print(world.display() + '\\n~~')\n",
    "\n",
    "                    logs = list()\n",
    "                    # time elapsed\n",
    "                    logs.append('time:{}s'.format(math.floor(train_time.time())))\n",
    "                    logs.append('parleys:{}'.format(parleys))\n",
    "                    if epochs_done > 0:\n",
    "                        logs.append('epochs done:{}'.format(epochs_done))\n",
    "\n",
    "                    # get report and update total examples seen so far\n",
    "                    if hasattr(agent, 'report'):\n",
    "                        train_report = agent.report()\n",
    "                        agent.reset_metrics()\n",
    "                    else:\n",
    "                        train_report = world.report()\n",
    "                        world.reset_metrics()\n",
    "\n",
    "                    if hasattr(train_report, 'get') and train_report.get('total'):\n",
    "                        total_exs += train_report['total']\n",
    "                        logs.append('total_exs:{}'.format(total_exs))\n",
    "\n",
    "                    # check if we should log amount of time remaining\n",
    "                    time_left = None\n",
    "                    if opt['num_epochs'] > 0 and total_exs > 0:\n",
    "                        exs_per_sec = train_time.time() / total_exs\n",
    "                        time_left = (max_exs - total_exs) * exs_per_sec\n",
    "                    if opt['max_train_time'] > 0:\n",
    "                        other_time_left = opt['max_train_time'] - train_time.time()\n",
    "                        if time_left is not None:\n",
    "                            time_left = min(time_left, other_time_left)\n",
    "                        else:\n",
    "                            time_left = other_time_left\n",
    "                    if time_left is not None:\n",
    "                        logs.append('time_left:{}s'.format(math.floor(time_left)))\n",
    "\n",
    "                    # join log string and add full metrics report to end of log\n",
    "                    log = '[ {} ] {}'.format(' '.join(logs), train_report)\n",
    "\n",
    "                    print(log)\n",
    "                    log_time.reset()\n",
    "\n",
    "                if 0 < opt['validation_every_n_secs'] < validate_time.time() or \\\n",
    "                        (opt['validation_every_n_epochs'] > 0 and new_epoch and (\n",
    "                                    epochs_done % opt['validation_every_n_epochs']) == 0):\n",
    "\n",
    "                    valid_report, valid_world = run_eval(agent, opt, 'valid',\n",
    "                                                         opt['validation_max_exs'],\n",
    "                                                         valid_world=valid_world)\n",
    "                    if best_metric_name not in valid_report and 'accuracy' in valid_report:\n",
    "                        best_metric_name = 'accuracy'\n",
    "                    if valid_report[best_metric_name] > best_metric:\n",
    "                        best_metric = valid_report[best_metric_name]\n",
    "                        impatience = 0\n",
    "                        lr_drop_impatience = 0\n",
    "                        print('[ new best ' + best_metric_name + ': ' + str(best_metric) + ' ]')\n",
    "                        world.save_agents()\n",
    "                        saved = True\n",
    "                        if best_metric == 1:\n",
    "                            print('[ task solved! stopping. ]')\n",
    "                            break\n",
    "                    else:\n",
    "                        impatience += 1\n",
    "                        lr_drop_impatience +=1\n",
    "                        print('[ did not beat best ' + best_metric_name + ': {} impatience: {} ]'.format(\n",
    "                                round(best_metric, 4), impatience))\n",
    "                    validate_time.reset()\n",
    "                    if 0 < opt['validation_patience'] <= impatience:\n",
    "                        print('[ ran out of patience! stopping training. ]')\n",
    "                        break\n",
    "                    if 'lr_drop_patience' in opt and 0 < opt['lr_drop_patience'] <= lr_drop_impatience:\n",
    "                        if hasattr(agent, 'drop_lr'):\n",
    "                            print('[ validation metric is decreasing, dropping learning rate ]')\n",
    "                            train_report = agent.drop_lr()\n",
    "                            agent.reset_metrics()\n",
    "                        else:\n",
    "                            print('[ there is no drop_lr method in agent, ignoring ]')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print('Stopped training, starting testing')\n",
    "\n",
    "        if not saved:\n",
    "            world.save_agents()\n",
    "        # else:\n",
    "        world.shutdown()\n",
    "\n",
    "        # reload best validation model\n",
    "        opt['pretrained_model'] = opt['model_file']\n",
    "        agent = create_agent(opt)\n",
    "\n",
    "        run_eval(agent, opt, 'valid', write_log=True)\n",
    "        run_eval(agent, opt, 'test', write_log=True)\n",
    "    else:\n",
    "        run_eval(agent, opt, opt['datatype'], write_log=True)\n",
    "    agent.shutdown()\n",
    "\n",
    "def train_cross_valid(opt):\n",
    "    if opt.get('model_files'):\n",
    "        opt['model_files'] = [fname+'_'+str(i) for fname in opt['model_files']\n",
    "                              for i in range(opt['cross_validation_splits_count'])]\n",
    "        train_model(opt)\n",
    "        return\n",
    "    for i in range(opt['cross_validation_splits_count']):\n",
    "        print(\"Training fold number %i\" % (i+1))\n",
    "        local_opt = copy.deepcopy(opt)\n",
    "        local_opt['model_file'] = opt.get('model_file', '') + '_' + str(i)\n",
    "        local_opt['cross_validation_model_index'] = i\n",
    "        train_model(local_opt)\n",
    "\n",
    "def main(args=None):\n",
    "    # Get command line arguments\n",
    "    parser = ParlaiParser(True, True)\n",
    "    train = parser.add_argument_group('Training Loop Arguments')\n",
    "    train.add_argument('-et', '--evaltask',\n",
    "                        help=('task to use for valid/test (defaults to the ' +\n",
    "                              'one used for training if not set)'))\n",
    "    train.add_argument('-d', '--display-examples',\n",
    "                        type='bool', default=False)\n",
    "    train.add_argument('-e', '--num-epochs', type=float, default=-1)\n",
    "    train.add_argument('-ttim', '--max-train-time',\n",
    "                        type=float, default=-1)\n",
    "    train.add_argument('-ltim', '--log-every-n-secs',\n",
    "                        type=float, default=2)\n",
    "    train.add_argument('-le', '--log-every-n-epochs',\n",
    "                        type=int, default=0)\n",
    "    train.add_argument('-vtim', '--validation-every-n-secs',\n",
    "                        type=float, default=-1)\n",
    "    train.add_argument('-ve', '--validation-every-n-epochs',\n",
    "                        type=int, default=0)\n",
    "    train.add_argument('-vme', '--validation-max-exs',\n",
    "                        type=int, default=-1,\n",
    "                        help='max examples to use during validation (default ' +\n",
    "                             '-1 uses all)')\n",
    "    train.add_argument('-vp', '--validation-patience',\n",
    "                        type=int, default=5,\n",
    "                        help=('number of iterations of validation where result '\n",
    "                              + 'does not improve before we stop training'))\n",
    "    train.add_argument('-dbf', '--dict-build-first',\n",
    "                        type='bool', default=True,\n",
    "                        help='build dictionary first before training agent')\n",
    "    train.add_argument('--chosen-metric', default='accuracy',\n",
    "                       help='metric with which to measure improvement')\n",
    "    train.add_argument('--lr-drop', '--lr-drop-patience', type=int, default=-1,\n",
    "                       help='drop learning rate if validation metric is not improving')\n",
    "    opt = parser.parse_args(args=args)\n",
    "    if opt.get('cross_validation_splits_count', 0) > 1 and opt.get('cross_validation_model_index') is None:\n",
    "        train_cross_valid(opt)\n",
    "    else:\n",
    "        train_model(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_agent(opt):\n",
    "    # Get command line arguments\n",
    "    parser = ParlaiParser(True, True)\n",
    "    train = parser.add_argument_group('Training Loop Arguments')\n",
    "    train.add_argument('-et', '--evaltask',\n",
    "                        help=('task to use for valid/test (defaults to the ' +\n",
    "                              'one used for training if not set)'))\n",
    "    train.add_argument('-d', '--display-examples',\n",
    "                        type='bool', default=False)\n",
    "    train.add_argument('-e', '--num-epochs', type=float, default=-1)\n",
    "    train.add_argument('-ttim', '--max-train-time',\n",
    "                        type=float, default=-1)\n",
    "    train.add_argument('-ltim', '--log-every-n-secs',\n",
    "                        type=float, default=2)\n",
    "    train.add_argument('-le', '--log-every-n-epochs',\n",
    "                        type=int, default=0)\n",
    "    train.add_argument('-vtim', '--validation-every-n-secs',\n",
    "                        type=float, default=-1)\n",
    "    train.add_argument('-ve', '--validation-every-n-epochs',\n",
    "                        type=int, default=0)\n",
    "    train.add_argument('-vme', '--validation-max-exs',\n",
    "                        type=int, default=-1,\n",
    "                        help='max examples to use during validation (default ' +\n",
    "                             '-1 uses all)')\n",
    "    train.add_argument('-vp', '--validation-patience',\n",
    "                        type=int, default=5,\n",
    "                        help=('number of iterations of validation where result '\n",
    "                              + 'does not improve before we stop training'))\n",
    "    train.add_argument('-dbf', '--dict-build-first',\n",
    "                        type='bool', default=True,\n",
    "                        help='build dictionary first before training agent')\n",
    "    train.add_argument('--chosen-metric', default='accuracy',\n",
    "                       help='metric with which to measure improvement')\n",
    "    train.add_argument('--lr-drop', '--lr-drop-patience', type=int, default=-1,\n",
    "                       help='drop learning rate if validation metric is not improving')\n",
    "    opt = parser.parse_args(args=args)\n",
    "    \n",
    "    \n",
    "    agent = create_agent(opt)\n",
    "    world = create_task(opt, agent)\n",
    "    my_conll_file = '/home/petrov/coreference_kpi/coreference/src/parlai/data/coreference/russian/test/1.russian.v4_conll'\n",
    "    conll_0 = conll2dict(0, my_conll_file, 'I', 'test', '1.russian.v4_conll')\n",
    "    agent.obs(conll_0)\n",
    "    conll_1 = agent.predict()\n",
    "    agent.shutdown()\n",
    "    \n",
    "    dict2conll(conll_1,'./new.conll')\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-t TASK] [--download-path DOWNLOAD_PATH]\n",
      "                             [-dt {train,train:ordered,valid,test}]\n",
      "                             [-im IMAGE_MODE] [-nt NUMTHREADS] [-bs BATCHSIZE]\n",
      "                             [-dp DATAPATH] [-m MODEL] [-mf MODEL_FILE]\n",
      "                             [--dict-class DICT_CLASS] [-et EVALTASK]\n",
      "                             [-d DISPLAY_EXAMPLES] [-e NUM_EPOCHS]\n",
      "                             [-ttim MAX_TRAIN_TIME] [-ltim LOG_EVERY_N_SECS]\n",
      "                             [-le LOG_EVERY_N_EPOCHS]\n",
      "                             [-vtim VALIDATION_EVERY_N_SECS]\n",
      "                             [-ve VALIDATION_EVERY_N_EPOCHS]\n",
      "                             [-vme VALIDATION_MAX_EXS]\n",
      "                             [-vp VALIDATION_PATIENCE] [-dbf DICT_BUILD_FIRST]\n",
      "                             [--chosen-metric CHOSEN_METRIC]\n",
      "                             [--lr-drop LR_DROP]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1600400027/jupyter/kernel-33509f72-6fcb-4270-bca0-cbcf270f6d11.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petrov/anaconda/envs/deeppavlov/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
